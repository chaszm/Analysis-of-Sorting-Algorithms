# Optimal Sorting Algorithms for Varying Datasets


# Introduction and Project Overview

Sorting algorithms are an integral part of computer science with many real world applications. Our group was drawn to this topic because we know just how important learning how to utilize these algorithms is for our future jobs. These types of algorithms allow programmers to simplify complex problems, organize large data sets, and improve the efficiency of any program. By mastering sorting algorithms, programmers gain the ability to tackle intricate problems and overall enhance the efficiency and performance of their programs. Real world examples of these algorithms are seen within our everyday lives with Google Search or in E-Commerce stores.  This shows just how relevant this topic is in the field of computer science. 

Our project, Optimal Sorting Algorithms for Varying Datasets, had us focusing on analyzing six different sorting algorithms. The sorting algorithms that we chose to implement during this project were Insertion sort, Merge Sort, Heap Sort, Radix Sort, Quick Sort, and Bubble Sort. We decided on these six algorithms because they are very commonly used within the field of computer science. Throughout our project, we focused on fully understanding each algorithm, how they perform, when we should implement them, and how they compare to each other. Through trial and error we managed to come up with a strategy to accurately record the data from each algorithm in order to compare them to one another. In the end, we accurately implemented, analyzed, and compared each of our algorithms through the data that we compiled.


# Aims and Objectives

Our group had a pretty straight forward plan to accurately analyze our algorithms. We quickly came up with a roadmap that had clear cut goals which assisted us with the completion of our task. The goals that we set were: Implementation of the algorithms, Analysis of each algorithm including time and space complexity, and finally compare and contrast each of our algorithms with each other. This set of clear goals helped our group to stay on the right path and achieve completion of our project. 

We started by splitting up the work between the three group members. Each member was assigned two algorithms to implement. We thought that splitting it up in this manner would allow us to quickly progress to the next goal that we had set. We decided to use Java as our programming language for this project as we are all very comfortable with the language. Once each member had successfully implemented, the next step was to analyze each algorithm. We again stuck to our two algorithms per group member. The first step of analysis was to gather the data for each algorithm. Once we developed a method to gather this data, we then created graphs for each sort. For the last step of analysis, we computed the time and space complexity for each of our sorts. The last goal that we had set was to compare each of our algorithms in order to determine similarities and differences between them. We did this by comparing the graphs, time and space complexities, and finally by the class of functions that each algorithm fell into. Overall, we set clear cut goals to implement, analyze, and compare our algorithms to one another. Without these goals, we would have lacked much of the needed efficiency and quality that this project required.


# Methodologies - System Configuration and Setup

The testing began with a setup of the environment. This is important since it ensures precision and consistency in our performance analysis of the six algorithms. The hardware configuration includes a Ryzen 7 5800x AMD processor which has 8 cores and a clock speed of 3.6 GHz. The RAM used was 16GB of DDR4 RAM, with a speed of 3200 MHz. The storage was an NVMe SSD. After some discussion, this system was chosen to reduce any potential hardware-induced inconsistencies in our algorithm performance data. The hardware is important to note since the algorithms will perform differently from system to system while still maintaining the worst-case time complexity.
 
     As far as software, the tests were done using Windows 10 Home. The Java Development Kit (JDK) 20 was used for each of the programs, another measure of consistency. To ensure a stable and consistent testing environment, most startup programs were disabled and background processes, unless insignificant, were manually ended. If such measures were not taken, any sort of background task running could have affected the data, producing inconsistent results.
 
     In addition to the hardware and software setup, we monitored the system's performance to ensure that no external factors influenced our algorithm testing. Attention was also given to network connectivity, ensuring that no background updates or other processes interfered during the testing. The idea behind the setup was to create an environment that operates under optimal conditions. These steps provided results that are both reliable and replicable in similar mid to high-performance systems.

# Methodologies - Benchmarking Procedures

Our benchmarking process was made to capture consistent and reliable data across all tested sorting algorithms. We conducted each algorithm test 30 times for every array size, which allowed us to account for any natural variability in the execution times. Outliers were identified and excluded to maintain the integrity of our data. To ensure consistency during testing, we did a JVM warm-up before each algorithm of ten tests before recording the actual data. This approach was important since it further reduced outlier data by ensuring similar caching behavior.

 The software tools used for these tests were equally significant. IntelliJ IDEA served as our platform for code compilation and execution. The data was recorded into a CSV file from the buffered writer and transferred to Excel after testing to form the graphs. A key aspect of our methodology was the standardized coding logic across all of our algorithm implementations. This standardization includes data generation, performance measurement, and main methods, ensuring that each algorithm was tested under similar conditions and with the same requirements. Each main method consists of the same for-loop utilizing a buffered writer.

The benchmarking procedures also included documentation. Each step of the testing, from initialization to completion, was recorded by screenshot. This documentation not only provided a reference for replicating similar test results but was also useful as a tool for identifying any outliers or inconsistencies in the process. By keeping a record of the testing procedures, we aimed for a high standard of transparency and methodology, which is something that is essential in a scientific inquiry.

      
# Methodologies - Algorithms and Data Sets

 In this project, we aimed to cover a broad spectrum of sorting algorithms, each chosen for its characteristics, performance, and relevance. Our selection included bubble sort, heap sort, insertion sort, merge sort, quick sort, and radix sort. There is also a broad range of time complexity. This variety was a factor in making the decision of which algorithms to include and comparisons across different time complexity classes.

 The data sets used were also an important decision. We utilized arrays of integers, randomly generated, with sizes varying from 10 elements 25,000, for a total of 12 different sizes. This range was chosen to avoid memory overflow issues while still providing a broad spectrum for analysis. Quick sort unfortunately provided an issue for us and we were presented with a stack overflow error. This algorithm was tested with array sizes up to 3,000. We believe this changed implementation is a better representation of its worst-case screnario. The data sets were standardized with a max element value of 100,000, which was another measure of uniformity.

Furthermore, the algorithms were chosen to represent a variety array of sorting methodologies, including comparison-based and non-comparison-based sorts. This variety allowed us to look at the nuances of each algorithm's performance across randomized data sets. For example, while comparison-based sorts like insertion sort and merge sort provide insight into more traditional algorithm efficiency, non-comparison sorts like radix sort offer a perspective on how an alternative approach can optimize sorting under specific conditions. This holistic approach to our algorithm selection underscores our idea of providing a comprehensive analysis of sorting algorithm performance.
