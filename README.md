# Optimal Sorting Algorithms for Varying Datasets


# Introduction and Project Overview

Sorting algorithms are an integral part of computer science with many real world applications. Our group was drawn to this topic because we know just how important learning how to utilize these algorithms is for our future jobs. These types of algorithms allow programmers to simplify complex problems, organize large data sets, and improve the efficiency of any program. By mastering sorting algorithms, programmers gain the ability to tackle intricate problems and overall enhance the efficiency and performance of their programs. Real world examples of these algorithms are seen within our everyday lives with Google Search or in E-Commerce stores.  This shows just how relevant this topic is in the field of computer science. 

Our project, Optimal Sorting Algorithms for Varying Datasets, had us focusing on analyzing six different sorting algorithms. The sorting algorithms that we chose to implement during this project were Insertion sort, Merge Sort, Heap Sort, Radix Sort, Quick Sort, and Bubble Sort. We decided on these six algorithms because they are very commonly used within the field of computer science. Throughout our project, we focused on fully understanding each algorithm, how they perform, when we should implement them, and how they compare to each other. Through trial and error we managed to come up with a strategy to accurately record the data from each algorithm in order to compare them to one another. In the end, we accurately implemented, analyzed, and compared each of our algorithms through the data that we compiled.


# Aims and Objectives

Our group had a pretty straight forward plan to accurately analyze our algorithms. We quickly came up with a roadmap that had clear cut goals which assisted us with the completion of our task. The goals that we set were: Implementation of the algorithms, Analysis of each algorithm including time and space complexity, and finally compare and contrast each of our algorithms with each other. This set of clear goals helped our group to stay on the right path and achieve completion of our project. 

We started by splitting up the work between the three group members. Each member was assigned two algorithms to implement. We thought that splitting it up in this manner would allow us to quickly progress to the next goal that we had set. We decided to use Java as our programming language for this project as we are all very comfortable with the language. Once each member had successfully implemented, the next step was to analyze each algorithm. We again stuck to our two algorithms per group member. The first step of analysis was to gather the data for each algorithm. Once we developed a method to gather this data, we then created graphs for each sort. For the last step of analysis, we computed the time and space complexity for each of our sorts. The last goal that we had set was to compare each of our algorithms in order to determine similarities and differences between them. We did this by comparing the graphs, time and space complexities, and finally by the class of functions that each algorithm fell into. Overall, we set clear cut goals to implement, analyze, and compare our algorithms to one another. Without these goals, we would have lacked much of the needed efficiency and quality that this project required.


# Methodologies - System Configuration and Setup

The testing began with a setup of the environment. This is important since it ensures precision and consistency in our performance analysis of the six algorithms. The hardware configuration includes a Ryzen 7 5800x AMD processor which has 8 cores and a clock speed of 3.6 GHz. The RAM used was 16GB of DDR4 RAM, with a speed of 3200 MHz. The storage was an NVMe SSD. After some discussion, this system was chosen to reduce any potential hardware-induced inconsistencies in our algorithm performance data. The hardware is important to note since the algorithms will perform differently from system to system while still maintaining the worst-case time complexity.

 As far as software, the tests were done using Windows 10 Home. The Java Development Kit (JDK) 20 was used for each of the programs, another measure of consistency. To ensure a stable and consistent testing environment, most startup programs were disabled and background processes, unless insignificant, were manually ended. If such measures were not taken, any sort of background task running could have affected the data, producing inconsistent results.

In addition to the hardware and software setup, we monitored the system's performance to ensure that no external factors influenced our algorithm testing. Attention was also given to network connectivity, ensuring that no background updates or other processes interfered during the testing. The idea behind the setup was to create an environment that operates under optimal conditions. These steps provided results that are both reliable and replicable in similar mid to high-performance systems.
 
# Methodologies - Benchmarking Procedures

Our benchmarking process was made to capture consistent and reliable data across all tested sorting algorithms. We conducted each algorithm test 30 times for every array size, which allowed us to account for any natural variability in the execution times. Outliers were identified and excluded to maintain the integrity of our data. To ensure consistency during testing, we did a JVM warm-up before each algorithm of ten tests before recording the actual data. This approach was important since it further reduced outlier data by ensuring similar caching behavior.

 The software tools used for these tests were equally significant. IntelliJ IDEA served as our platform for code compilation and execution. The data was recorded into a CSV file from the buffered writer and transferred to Excel after testing to form the graphs. A key aspect of our methodology was the standardized coding logic across all of our algorithm implementations. This standardization includes data generation, performance measurement, and main methods, ensuring that each algorithm was tested under similar conditions and with the same requirements. Each main method consists of the same for-loop utilizing a buffered writer.

The benchmarking procedures also included documentation. Each step of the testing, from initialization to completion, was recorded by screenshot. This documentation not only provided a reference for replicating similar test results but was also useful as a tool for identifying any outliers or inconsistencies in the process. By keeping a record of the testing procedures, we aimed for a high standard of transparency and methodology, which is something that is essential in a scientific inquiry.

      
# Methodologies - Algorithms and Data Sets

 In this project, we aimed to cover a broad spectrum of sorting algorithms, each chosen for its characteristics, performance, and relevance. Our selection included bubble sort, heap sort, insertion sort, merge sort, quick sort, and radix sort. There is also a broad range of time complexity. This variety was a factor in making the decision of which algorithms to include and comparisons across different time complexity classes.

 The data sets used were also an important decision. We utilized arrays of integers, randomly generated, with sizes varying from 10 elements 25,000, for a total of 12 different sizes. This range was chosen to avoid memory overflow issues while still providing a broad spectrum for analysis. Quick sort unfortunately provided an issue for us and we were presented with a stack overflow error. This algorithm was tested with array sizes up to 3,000. We believe this changed implementation is a better representation of its worst-case screnario. The data sets were standardized with a max element value of 100,000, which was another measure of uniformity.

Furthermore, the algorithms were chosen to represent a variety array of sorting methodologies, including comparison-based and non-comparison-based sorts. This variety allowed us to look at the nuances of each algorithm's performance across randomized data sets. For example, while comparison-based sorts like insertion sort and merge sort provide insight into more traditional algorithm efficiency, non-comparison sorts like radix sort offer a perspective on how an alternative approach can optimize sorting under specific conditions. This holistic approach to our algorithm selection underscores our idea of providing a comprehensive analysis of sorting algorithm performance.


# Results

  The first part of the results section involves looking into the performance of two specific algorithms: insertion sort and merge sort. The insertion sort algorithm, as observed in our tests, displayed an efficiency that scaled down with the increase in array size. For smaller data sets, its execution time was negligible, showing efficiency. However, as the array size crossed larger element array sizes, the quadratic nature of its time complexity O(n^2) became more evident, leading to significantly longer execution times for larger arrays. The insertion sort implementation has a space complexity of O(1) as it only uses a fixed number of variables for its operations, irrespective of the size of the input array. The minimal space requirement shows its usefulness in scenarios where memory is more limited.

In contrast, Merge Sort exhibited a consistent linearithmic time complexity, O(nlogn), across the tested data sizes. This algorithm proved to have consistency and efficiency, especially in handling larger data sets, where its stable execution times highlighted that it is reliable and predictable. Merge sort is also tight bound, meaning that its upper bound and lower bound are the same. However, the merge sort implementation requires O(n) space complexity because it allocates additional arrays (left and right) proportional to the size of the input array for the purpose of merging sorted subarrays. This is one drawback for merge sort.

In a comparative sense, while insertion sort held its ground for smaller data sizes, merge sort emerged as a more scalable and efficient solution for handling larger arrays. This comparative analysis, along with graphical representations, is relevant in demonstrating the practical implications of what seemed like only theoretical time complexities in real-world scenarios.

The second part of the results section focuses on our next two sorting algorithms: Heap sort and Radix sort. Based on our findings, the heap sort algorithm is a divide and conquer algorithm whose efficiency is based on the overall input size (n). As the input size grows, heap sort begins to lose efficiency and get closer to its overall time complexity of O(nlogn). Based on this, we determined that heap sort belongs within the Logarithmic Class of functions. The space complexity of this algorithm is O(1). This is because Heap sort sorts a list in-place and therefore uses a constant amount of space. It achieves this through sorting a list in-place using a heap structure and then adding the max value to the end of the same list, so only one list is utilized in the sort. 

The next algorithm is Radix sort. Unlike heap sort, which utilizes a divide and conquer strategy, radix sort utilizes a decrease and conquer strategy.  This is because the radix sort algorithm decreases its problem down to sorting by individual digits. It begins with either the least or most significant digits and sorts its way through. Once sorted, the elements are combined to form a sorted list. The time complexity of this sort is O(n+k), however, we found that because it sorts each digit independently the time complexity is O(d * (n+k)). However, because d is often a small constant value, we concluded that this sort belongs within the linearithmic class of functions. The space complexity of radix sort is O(n).  This is because during each pass of the algorithm, data is stored in a temporary space based on their digit values and then combined later. 

The next algorithm that was tested is quick sort. Quick sort is an efficient, general- purpose sorting algorithm based on the divide and conquer approach. Our implementation of this algorithm involves first choosing an element to be a pivot and partitioning the elements based on the value of the pivot element. The dataset is then partitioned into two subarrays, where elements in the set that are less than or equal to the pivot are put on the left, while elements greater than or equal to the pivot are put on the right. This process is then recursively applied to both subarrays, which will further sort the dataset. Note that the pivot can be any element in the set, but our implementation chooses the pivot to be the last (rightmost) element in the array. 

The implementation of our algorithm was tested with random datasets of different sizes. Although quick sort is generally efficient, it can be slow if the dataset is sorted in a way that the pivot creates two unbalanced segments. This makes the algorithm inefficient, as more steps and sub arrays have to be done to properly sort the dataset. Our results showed that the algorithm was extremely efficient with smaller datasets, but began to somewhat decrease in efficiency for larger sets. Quick sort has a time complexity of O(n^2), which could be seen in our data with the tests of larger datasets. It is in the quadratic class of functions, which our result graphs depict as the input size increased. The space complexity of quick sort is O(logn), which is extremely efficient. This is because additional space that is needed is only created from the recursive function calls and the associated call stack, leading to a small amount of memory space usage.



Bubble sort is a very simple sorting algorithm that is based on the brute-force (incremental) approach. Our bubble sort implementation was fairly simple, and was tested with small and large datasets. It involves continuously comparing and swapping two adjacent elements in the dataset. This continues until the entire list is sorted and the elements are in the correct order. The implementation of our algorithm was done through  continuous comparisons and swaps with nested for loops. Bubble sort is efficient with small and/or almost sorted datasets, but becomes extremely inefficient at larger sets, as it uses the brute force approach.

Our data depicted the huge increase in execution time with larger datasets, depicting the quadratic class of functions. The time complexity of  bubble sort is O(n^2), which was readily apparent at the higher values of  N.  In regards to our implementation, the time complexity was calculated by using the summation method on the nested for loops. Although bubble sort is quick and efficient for the smaller value datasets in our implementation, the worst case time complexity of O(n^2) is seen as the dataset grows larger. The main advantage to using bubble sort is its simplicity, as it is both easy to implement and understand. The space complexity of bubble sort is O(1), as the algorithm uses a constant amount of space for temporary variables, and it is not affected by the size of the input. Bubble sort is an in-place sorting algorithm, which means that it performs all its operations directly on the input array, which results in the O(1) space complexity.


# Conclusion 

In conclusion, the choice of sorting algorithm plays a crucial role in optimizing the performance of various applications that involve organizing data. Each sorting algorithm has its strengths and weaknesses, making them suitable for different scenarios. For small or nearly sorted datasets, Bubble Sort emerges as a simple and easily understood choice. Its straightforward implementation makes it ideal, although it may not scale well for larger datasets. For average-case efficiency in randomized, unsorted data, Merge Sort and QuickSort stand out as the most efficient algorithms. The divide-and-conquer strategy that is used contributes to efficient sorting, and their performance is often superior to other algorithms on a broad range of inputs. Insertion Sort, with its adaptability to nearly sorted datasets, shines when dealing with smaller data sets that may already be partially sorted. This characteristic makes Insertion Sort a good choice in situations where the dataset exhibits some pre-existing order. Radix Sort demonstrates its strength when handling large numerical datasets. Its ability to sort data by digits or other significant components makes it particularly effective for scenarios where the key is a numerical value. Heap Sort, known for its simplicity and efficiency, is advantageous for large datasets. Its in-place sorting contributes to its reliability and resource management and makes it a suitable option for applications dealing with large amounts of data.

 In the broader context, the understanding and selection of sorting algorithms contribute significantly to many computing concepts. Concepts such as asymptotic time and space complexity become easier to understand through the analysis and application of these algorithms, improving the ability to design and analyze more complex algorithms. Today, sorting algorithms continue to play a vital role in databases, user interfaces, and other computational domains.
